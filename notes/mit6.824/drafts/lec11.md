# LEC 11: Cache Consistency: Frangipani

<!-- @import "[TOC]" {cmd="toc" depthFrom=2 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [NotebookLM 生成的双人播客](#notebooklm-生成的双人播客)
- [Gemini 2.5 Pro 生成的“Frangipani 分布式文件系统深度解析：缓存一致性、分布式事务和分布式崩溃恢复”](#gemini-25-pro-生成的frangipani-分布式文件系统深度解析缓存一致性-分布式事务和分布式崩溃恢复)
  - [宏观架构：聪明的“客户端”与简单的“存储”](#宏观架构聪明的客户端与简单的存储)
    - [Frangipani 与 GFS 的核心区别](#frangipani-与-gfs-的核心区别)
    - [为什么 Petal 提供的是“块”接口？](#为什么-petal-提供的是块接口)
  - [核心挑战与 Frangipani 的解法](#核心挑战与-frangipani-的解法)
    - [挑战一：缓存一致性 (Cache Coherence)](#挑战一缓存一致性-cache-coherence)
      - [什么是“伪共享 (false sharing)”问题？](#什么是伪共享-false-sharing问题)
    - [挑战二：原子性与分布式事务](#挑战二原子性与分布式事务)
      - [Frangipani vs. 两阶段提交 (Two-Phase Commit)](#frangipani-vs-两阶段提交-two-phase-commit)
    - [挑战三：崩溃恢复 (Crash Recovery)](#挑战三崩溃恢复-crash-recovery)
      - [用户文件内容会丢失吗？](#用户文件内容会丢失吗)
      - [如何保证日志重放的正确性？](#如何保证日志重放的正确性)
  - [其他值得探讨的问题](#其他值得探讨的问题)
    - [安全性问题](#安全性问题)
    - [性能考量](#性能考量)
    - [Petal 的快照机制](#petal-的快照机制)
    - [Frangipani 的现状与启示](#frangipani-的现状与启示)

<!-- /code_chunk_output -->

## NotebookLM 生成的双人播客

注：播客里的“欢权”实际上是“缓存”， AI 读错了

https://notebooklm.google.com/notebook/bbd06c96-41fd-4625-892b-3ab726d7732a/audio

微信链接： https://mp.weixin.qq.com/s/WncPxJ4slEFNHbpr2H7XuQ

给我留下的较深的印象：
1. 伪共享 (false sharing) 实际上是解决数据的分片
2. 依靠“预写式日志 (Write-Ahead Logging, WAL)”来把操作写入 Petal （一个高可用日志服务），这样来保证崩溃恢复；引入元数据块（比如 inode）版本号来保证回放正确性
3. 只记录元数据 (metadata) 的变更（只保证元数据的原子性），写入文件数据需要用户确保其正确性，这和 Linux/Unix 系统的设计是一致的

## Gemini 2.5 Pro 生成的“Frangipani 分布式文件系统深度解析：缓存一致性、分布式事务和分布式崩溃恢复”

**Frangipani** 是一个诞生于 1997 年的分布式文件系统，由当时大名鼎鼎的 **数字设备公司（Digital Equipment Corporation, DEC）** 的研究员们打造。

我们为什么要在今天回顾这样一个“古老”的系统？因为它极其清晰地展示了构建一个分布式系统时必须面对的三个核心挑战—— **缓存一致性 (cache coherence)** 、 **分布式事务 (distributed transactions)** 和 **分布式崩溃恢复 (distributed crash recovery)** ，以及这三者之间错综复杂的“三角关系”。Frangipani 的设计就像一本教科书，把这些复杂问题掰开揉碎了呈现给我们。

### 宏观架构：聪明的“客户端”与简单的“存储”

Frangipani 的设计目标非常明确：为一组在同一个办公环境（比如一个实验室）里协同工作的工程师，提供一个统一、高性能、且支持共享的文件系统。用户可以在任何一台工作站上登录，都能看到完全一致的文件视图，就像在操作本地文件一样。

为了实现这个目标，Frangipani 采用了非常经典的两层架构：

```txt
+---------------------------------------------------+
|                   User Programs                   |
+---------------------------------------------------+
|      Workstations with Frangipani File System     |
+---------------------------------------------------+
|                      Network                      |
+---------------------------------------------------+
|         Petal Distributed Virtual Disk            |
+---------------------------------------------------+
```

1.  **底层是 Petal** ：一个分布式的虚拟磁盘服务。你可以把它想象成一个容量巨大、高可用的网络硬盘。它把文件系统的所有数据，包括目录、i-node、文件内容块、空闲块位图等，都以最原始的“块”形式储存起来。
2.  **上层是 Frangipani** ：它并不像传统文件系统那样有一个中心化的“文件服务器”。相反，文件系统的逻辑被分散到每一台运行 Frangipani 的工作站上。这些工作站都直接与底层的 Petal 通信，共享同一份数据。

#### Frangipani 与 GFS 的核心区别

这里可以和另一个著名的分布式文件系统 GFS (Google File System) 做个对比。它们的设计哲学截然不同：

* **逻辑分布** ：Frangipani 将文件系统的核心逻辑放在客户端（工作站），而 GFS 的逻辑绝大部分集中在 Master 和 Chunkserver 这样的服务器端。
* **缓存策略** ：Frangipani 为了性能，在每个工作站都设计了复杂的 **写回缓存 (write-back cache)** 。而 GFS 几乎没有客户端缓存，因为它主要为一次性、顺序读写TB级别的巨大文件而设计，缓存没有意义。
* **一致性** ：因为有缓存，Frangipani 必须实现一套复杂的 **缓存一致性协议** 来保证数据的一致性。而 GFS 因为没有缓存，也就不需要这个协议。
* **接口** ：Frangipani 对外呈现为一个标准的、与现有应用完全兼容的文件系统。而 GFS 需要应用专门通过其提供的库函数来访问。

#### 为什么 Petal 提供的是“块”接口？

你可能会问，既然最终需要的是文件和目录，为什么不让 Petal 直接提供文件服务（像 AFS 那样）呢？

这背后主要有两个原因。首先，Petal 这个项目本身是先于 Frangipani 开发的，它已经解决了存储层的扩展性和容错问题。直接复用 Petal，大大简化了 Frangipani 的设计。其次，这种设计将计算密集型的文件系统逻辑从中心服务器转移到了各个客户端工作站，使得整个系统的性能可以随着工作站数量的增加而线性扩展。

-----

### 核心挑战与 Frangipani 的解法

Frangipani 的去中心化架构带来了高性能和高可扩展性，但同时也引出了三大严峻挑战。

#### 挑战一：缓存一致性 (Cache Coherence)

由于每个工作站都有自己的写回缓存，一个严峻的问题出现了：当工作站 A 修改了文件 `foo`，它的修改只是暂存在自己的缓存里，并没有立刻写回 Petal 。这时，如果工作站 B 去读取 `foo`，它会读到 Petal 上的旧数据吗？

为了解决这个问题，Frangipani 引入了一个独立的 **分布式锁服务 (lock service)** 。

* **基本原则** ：一个工作站想读写某个文件，必须先从锁服务那里获取该文件的锁。读操作需要 **读锁 (read lock)** ，写操作需要 **写锁 (write lock)** 。
* **协议流程** ：
  1.  **请求 (Request)** ：工作站 A 向锁服务请求文件 `z` 的写锁。
  2.  **授予 (Grant)** ：锁服务将 `z` 的所有权交给 A。A 从 Petal 读取 `z` 的数据到本地缓存，然后进行修改。
  3.  **撤销 (Revoke)** ：此时，工作站 B 也想读 `z`，向锁服务请求读锁。锁服务发现锁在 A 手里，于是向 A 发送一个“撤销”请求。
  4.  **释放 (Release)** ：A 收到撤销请求后，必须先将自己缓存中被修改过的（即“脏”的）`z` 数据写回 Petal，然后再向锁服务释放锁。
  5.  **再次授予** ：锁服务收到 A 的释放消息后，再将 `z` 的读锁授予 B。B 此时从 Petal 读取的就是最新的数据了。

通过这套锁机制，Frangipani 保证了任何读操作都能读到最近一次写操作的结果，实现了 **强一致性** 。

##### 什么是“伪共享 (false sharing)”问题？

在设计锁的粒度时，一个常见的问题是 **伪共享（false sharing）** 。Frangipani 的读写单位是 512 字节的块。如果两个不相关的数据（比如两个文件的 inode）被放在了同一个 512 字节的块里，那么当两个工作站分别想修改这两个不相关的数据时，它们却不得不争抢同一个块的锁，来回传递数据块，造成不必要的性能开销。Frangipani 通过一个简单的策略避免了这个问题： **让每个 inode 单独占用一个 512 字节的块** 。虽然有点浪费空间，但极大地简化了设计，避免了伪共享。

#### 挑战二：原子性与分布式事务

当多个工作站同时操作同一个目录时，比如同时在根目录下创建文件 `/a` 和 `/b`，如何保证目录的最终状态是正确的，而不会因为并发修改导致数据损坏？

Frangipani 巧妙地复用了它的锁机制来实现 **事务性操作 (transactional operations)** 。一个文件系统操作（如创建文件）可能涉及多个步骤（分配 inode、初始化 inode、更新目录数据块）。Frangipani 的做法是：

1.  在执行操作前，工作站必须 **获取所有** 将要被修改的数据块的锁。
2.  在 **持有所有锁** 的情况下，完成所有的修改。
3.  操作完成后，才释放这些锁。在持有锁期间，它不会响应锁服务的撤销请求。

这样一来，任何一个多步骤的操作，对于其他工作站来说都是 **原子 (atomic)** 的。它们要么看到操作完成后的最终状态，要么什么都没看到，绝不会看到一个只进行了一半的“中间状态” 。

##### Frangipani vs. 两阶段提交 (Two-Phase Commit)

Frangipani 的这种方式和数据库中经典的 **两阶段提交（Two-Phase Commit, 2PC）** 有相似之处，都是先加锁，再执行，最后解锁。但它们有本质区别：

* 在 Frangipani 中，锁和数据是“可移动的”，执行操作的工作站会把所有需要的锁和数据都“拉”到自己本地进行处理。
* 在典型的 2PC 系统中，数据是分片 (sharded) 存储在不同服务器上的，工作也必须被拆分到各个服务器上执行。
* 更重要的是，Frangipani 对于节点崩溃的处理更优雅。如果一个持有锁的工作站崩溃了，因为它的日志在共享的 Petal 上，其他工作站可以接管并完成恢复。而传统的 2PC，如果一个参与者的日志在本地磁盘上且该机器宕机，整个事务就会被阻塞，直到它重启。

#### 挑战三：崩溃恢复 (Crash Recovery)

最棘手的问题来了：如果一个工作站 A 在操作进行到一半时突然崩溃了，怎么办？它可能已经将一部分修改写回了 Petal，留下一个不一致的烂摊子。

Frangipani 的答案是： **预写式日志 (Write-Ahead Logging, WAL)** 。

* **独立的共享日志** ：Frangipani 的一个创新之处在于， **每个工作站都有自己独立的日志** ，并且这些日志都存储在共享的 Petal 虚拟磁盘上。
* **恢复流程** ：
  1.  当工作站 A 崩溃后，它持有的锁不会被立即释放。
  2.  当另一个工作站 B 需要 A 持有的锁时，锁服务会发现 A 已经失联（租约过期）。
  3.  锁服务会授权 B 去恢复 A 的状态。B 会读取 A 在 Petal 上的日志，并根据日志内容，重放（replay）那些 A 已经开始但可能未完成的操作，将它们彻底完成。
  4.  恢复完成后，B 通知锁服务，然后锁服务才会释放 A 的锁。

##### 用户文件内容会丢失吗？

这里需要强调，Frangipani 的日志 **只记录元数据 (metadata) 的变更** （比如目录项、inode），而不记录用户文件的实际内容。这意味着，如果一个程序写了一些数据到文件，然后工作站立刻崩溃，这些刚刚写入的数据可能会丢失。

这听起来很危险，但实际上，这和我们日常使用的标准 Unix/Linux 文件系统的行为是一样的。操作系统为了性能，并不会在每次 `write` 调用后都立刻把数据刷到磁盘。那些需要强持久性保证的程序，比如数据库或文本编辑器，会显式调用 `fsync()` 系统调用来强制数据落盘。Frangipani 的设计哲学是：文件系统负责保护自己内部结构的完整性；而用户数据的持久性，则交给应用程序自己来决定。

##### 如何保证日志重放的正确性？

一个非常精妙的问题是：由于各个工作站的日志是独立的，操作可能会交错发生。比如下面这个场景：

1.  `WS1`: `delete(d/f)`，然后崩溃。
2.  `WS2`: 在 `WS1` 崩溃后，`create(d/f)`，操作成功。
3.  `WS3`: 开始恢复 `WS1` 的日志。

`WS3` 在重放 `WS1` 的日志时，会看到一条 `delete(d/f)` 的记录。如果它无脑执行了这个删除操作，那就会错误地删除掉 `WS2` 刚刚创建的文件。

Frangipani 用一个叫做 **版本号 (version number)** 的机制完美解决了这个问题。

* 每个元数据块（比如 inode）都有一个版本号。
* 当一个操作要修改某个块时，它会在日志中记录下这个块的“新版本号”（通常是当前版本号 + 1）。
* 在恢复时，恢复进程只有在发现 **日志中记录的版本号 \> 磁盘上块的当前版本号** 时，才会执行重放操作。

在上面的例子中，当 `WS2` 创建 `d/f` 时，它已经更新了 `f` 的 inode，使其版本号增加。当 `WS3` 恢复 `WS1` 的日志时，它会发现 `WS1` 日志中 `delete` 操作记录的版本号，已经小于或等于磁盘上 inode 的当前版本号了。`WS3` 便知道这个 `delete` 操作已经被一个更新的操作（`WS2` 的 `create`）所覆盖，因此会安全地跳过它。

-----

### 其他值得探讨的问题

#### 安全性问题

既然文件系统的逻辑都在客户端，一个怀有恶意的用户是不是可以修改自己工作站上的 Frangipani 软件，然后为所欲为地读写 Petal 上的任何数据？

答案是 **肯定的** 。Frangipani 的设计基于一个核心假设：所有运行 Frangipani 的工作站都在一个可信的管理域内。因此，它不适合用户彼此不信任的环境。不过，可以通过一种“客户端/服务器”配置来解决这个问题：将 Frangipani 服务器部署在受保护的机器上，然后通过 NFS 等标准协议将文件系统导出给不受信任的客户端使用。

#### 性能考量

* **文件创建变慢** ：文件创建在 Frangipani 中会更耗时。这是因为元数据的修改需要 **写两次** ：一次写入日志，一次写入元数据本身在磁盘上的位置。此外，如果日志空间被写满，系统必须暂停，等待缓存中的脏数据被刷回 Petal，才能重用日志空间，这也会引入延迟。

* **锁的粒度** ：Frangipani 使用的是 **每个文件/目录一把锁** 的粗粒度锁定。如果换成更细的 **每块一把锁** ，性能会更好吗？答案是 **可能更差** 。对于工程类应用，常见的操作是整个文件的读写（如编译器读取源文件）。如果每读一个块都要申请一次锁，那么锁服务的开销和网络通信将是巨大的。

* **文件条带化 (Striping)** ：这和 **分片 (sharding)** 很相似。它指将一个大文件的不同数据块分散存储到多个不同的物理服务器上。这样做有两个好处：一是当读取这个大文件时，可以从多个服务器并行读取，获得极高的吞吐量；二是可以确保负载被均匀地分摊到各个服务器上。

#### Petal 的快照机制

Petal 提供了一个非常高效的 **快照 (snapshot)** 功能。它是如何做到的呢？Petal 内部维护着一个从“虚拟磁盘地址”到“物理磁盘地址”的映射表。这个映射表的索引不仅仅是虚拟块号，而是 **(虚拟块号, 时间戳纪元号)** 的一个组合对。

* 创建一个快照，操作极其简单： **只需将全局的“当前纪元号”加一** 。
* 当系统写入一个虚拟块时，它会检查该块现有映射的纪元号。如果小于当前的全局纪元号，Petal 会分配一个新的物理块，并用当前的纪元号创建一个新的映射，而旧的物理块和映射保持不变（这就是 copy-on-write）。
* 读取数据时，系统会使用具有最高纪元号的那个映射。读取一个历史快照，则只需指定该快照的纪元号即可。

#### Frangipani 的现状与启示

Frangipani 已经有二十多年的历史了。如今，分布式文件系统的版图已经发生了巨大变化。

* 云存储和大数据应用的兴起，使得存储系统的焦点发生了转移。
* 对于 Web 服务，键值存储（Key-Value Store）或数据库通常是比文件系统更好的选择。
* 对于大数据处理（如 MapReduce），系统更关心对巨大文件的并行吞吐能力，像 GFS 或 HDFS 这样的系统更受欢迎，它们不需要 Frangipani 这种复杂的缓存和锁机制。
* 尽管如此，像 SMB、NFS 这样的传统协议依然被广泛使用，而更新的系统如 Ceph、Lustre 也占据了一席之地。

Frangipani 虽然没有成为主流商业产品，但它所探索的“将智能分布在客户端，后端提供简单共享存储”的设计模式，以及它在一致性、原子性和恢复性之间所做的精妙权衡，至今仍然对我们设计和理解分布式系统有着深刻的启示。它是一座连接过去与未来的桥梁，展示了那些永恒的分布式计算难题和优雅的解决方案。
