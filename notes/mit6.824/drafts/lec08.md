# LEC 8: Zookeeper

<!-- @import "[TOC]" {cmd="toc" depthFrom=2 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [NotebookLM 生成的双人播客](#notebooklm-生成的双人播客)
- [Gemini 2.5 Pro 生成的 ZooKeeper 的设计精髓、工作原理和实际应用](#gemini-25-pro-生成的-zookeeper-的设计精髓-工作原理和实际应用)
  - [ZooKeeper 是什么？为什么要造这个轮子？](#zookeeper-是什么为什么要造这个轮子)
  - [核心设计：像文件系统一样简单](#核心设计像文件系统一样简单)
  - [核心机制：Watch 事件通知](#核心机制watch-事件通知)
  - [API 和保证：ZooKeeper 的契约](#api-和保证zookeeper-的契约)
    - [为什么读操作不保证线性一致性？](#为什么读操作不保证线性一致性)
  - [生产实践：用 ZooKeeper 搭建协调原语](#生产实践用-zookeeper-搭建协调原语)
    - [动态配置管理](#动态配置管理)
    - [服务发现与组成员管理](#服务发现与组成员管理)
    - [分布式锁（避免羊群效应）](#分布式锁避免羊群效应)
    - [领导者选举](#领导者选举)
  - [实际应用与常见问题](#实际应用与常见问题)
- [补充：ZooKeeper 核心实现原理以及一个系统与 ZooKeeper 的完整交互流程](#补充zookeeper-核心实现原理以及一个系统与-zookeeper-的完整交互流程)
  - [Part 1：ZooKeeper 核心实现原理](#part-1zookeeper-核心实现原理)
    - [1. 复制状态机与内存数据库](#1-复制状态机与内存数据库)
    - [2. Leader、Follower 与 Zab 协议](#2-leader-follower-与-zab-协议)
    - [3. 请求处理流程](#3-请求处理流程)
  - [Part 2：一个系统与 ZooKeeper 的完整交互流程](#part-2一个系统与-zookeeper-的完整交互流程)
    - [第 1 步：初始化 (管理员操作)](#第-1-步初始化-管理员操作)
    - [第 2 步：服务启动，参与选举 (所有服务器)](#第-2-步服务启动参与选举-所有服务器)
    - [第 3 步：判断谁是 Leader (所有服务器)](#第-3-步判断谁是-leader-所有服务器)
    - [第 4 步：Follower 等待 (Server-B, Server-C)](#第-4-步follower-等待-server-b-server-c)
    - [第 5 步：故障发生与自动切换](#第-5-步故障发生与自动切换)
    - [第 6 步：新 Leader 诞生](#第-6-步新-leader-诞生)
  - [总结](#总结)

<!-- /code_chunk_output -->

## NotebookLM 生成的双人播客

https://notebooklm.google.com/notebook/92c085a7-b32e-46a7-8a78-8449152c9f1e/audio

微信链接： https://mp.weixin.qq.com/s/CwxmxqPSzYE0H_Zuuv6mNg

给我留下的较深的印象：
1. 允许 Follower 承担读请求，大大降低负载，但是不保证读到最新，技术上的权衡
2. 分布式锁的实现中，客户端 A 获取 lock-1 ， B 获取 lock-2 ， C 获取 lock-3 ，这里是 C watch lock-2 、 B watch lock-1 ，来避免羊群效应
3. 安装 ZooKeeper 竟然需要 JVM ，让我有些吃惊，同时也惊叹于 java 在分布式系统中的生态位

## Gemini 2.5 Pro 生成的 ZooKeeper 的设计精髓、工作原理和实际应用

### ZooKeeper 是什么？为什么要造这个轮子？

想象一下，在一个大型分布式系统里，成百上千台服务器协同工作。这时候，会涌现出一大堆“鸡毛蒜皮”的协调问题：

* **配置管理 (Configuration Management)** ：某个配置项变了，如何让所有机器都收到最新的配置？
* **服务发现 (Service Discovery)** ：系统中谁是主服务器（Leader）？哪些工作节点（Worker）还活着？新上线的服务，它的地址和端口是什么？
* **分布式锁 (Distributed Lock)** ：多个进程要抢占同一个关键资源，怎么保证同一时间只有一个能拿到？
* **组成员管理 (Group Membership)** ：如何维护一个集群中所有在线成员的列表？

为每个问题都专门开发一套高可用的服务，不仅费时费力，而且很容易出错。ZooKeeper 的目标，就是提供一个通用的、高性能的“协调内核”。它本身不直接提供复杂的分布式锁或领导者选举等功能，而是提供一套足够基础、足够强大的 API，让开发者可以在它的基础上，像搭积木一样轻松构建出自己需要的、更复杂的协调“原语”（Primitives）。

它的设计哲学是“大道至简”：服务器端只做最核心、最简单的事，从而保证高性能和高可靠性，而将复杂性留给客户端去实现。

### 核心设计：像文件系统一样简单

ZooKeeper 对外暴露的接口非常像一个精简版的文件系统。它的核心数据模型是一个树状的层级命名空间，由许多被称为 **数据节点 (znodes)** 的单元组成。

```txt
/
├── app1
│   ├── p_1  (ephemeral)
│   ├── p_2  (ephemeral)
│   └── p_3  (ephemeral)
└── app2
    ├── config
    └── lock
        ├── write-0000000001
        └── read-0000000002
```

每个 znode 都可以存储少量数据（默认不超过 1MB），通常是用于协调的元数据，比如状态信息、配置参数或者节点地址。

Znode 有几种非常关键的类型：

* **常规节点 (Regular)** ：需要客户端显式地创建和删除。
* **临时节点 (Ephemeral)** ：这种节点的生命周期与创建它的客户端 **会话 (session)** 绑定。当客户端与 ZooKeeper 的连接断开，会话超时结束后，这个临时节点就会被自动删除。这个特性是实现服务发现和故障检测的利器。
* **顺序节点 (Sequential)** ：创建时，ZooKeeper 会在节点路径后面自动追加一个单调递增的数字序号。比如，在 `/app2/lock/` 下创建一个名为 `write-` 的顺序节点，可能会得到 `write-0000000001`、`write-0000000002` 这样的路径。这个特性对于实现分布式锁和队列至关重要，可以有效避免“羊群效应”。

### 核心机制：Watch 事件通知

如果客户端想知道某个 znode（比如存储着主节点地址的 znode）有没有变化，难道要不停地去轮询（Polling）读取吗？这显然效率低下，而且会给 ZooKeeper 服务带来巨大压力。

为此，ZooKeeper 引入了 **监视 (Watch)** 机制。客户端在读取一个 znode 时，可以设置一个 `watch` 标志。当这个 znode 发生变化（被修改、被删除，或者它的子节点列表发生变化）时，ZooKeeper 就会向该客户端发送一个一次性的通知。客户端收到通知后，就知道自己本地缓存的数据已经“过时”了，需要重新来拉取最新数据。

这个设计非常巧妙，它是一种事件驱动的机制，类似于缓存失效通知，避免了无效的轮询，大大提升了效率。

### API 和保证：ZooKeeper 的契约

ZooKeeper 提供了一套简洁的 API，核心包括：

* `create(path, data, flags)`: 创建一个 znode 。
* `delete(path, version)`: 删除一个 znode，`version` 参数用于实现乐观锁（CAS）。
* `exists(path, watch)`: 检查 znode 是否存在，并可以设置 watch 。
* `getData(path, watch)`: 获取 znode 的数据和元数据，并可以设置 watch 。
* `setData(path, data, version)`: 更新 znode 的数据，同样有 `version` 检查。
* `getChildren(path, watch)`: 获取子节点列表，并可以设置 watch 。
* `sync(path)`: 强制后续的读操作能看到此 `sync` 调用之前的所有更新。

在这些 API 背后，ZooKeeper 提供了两条黄金保证：

1.  **线性化写入 (Linearizable Writes)** ：所有会改变 ZooKeeper 状态的写操作，其执行顺序是全局一致、可串行化的，并且尊重操作的实际发生顺序。简单说，就是写操作绝不会乱序。这是通过一个类似 Raft 的原子广播协议 Zab 来实现的。
2.  **FIFO 客户端顺序 (FIFO Client Order)** ：来自同一个客户端的所有请求，会被严格按照它们发送的顺序来执行。这让异步操作变得简单可靠。比如，客户端可以先发一堆写请求去修改配置，最后发一个创建 "ready" 节点的请求，ZooKeeper 保证 "ready" 节点一定是在所有配置修改完成后才出现的。

#### 为什么读操作不保证线性一致性？

这里有一个关键的设计取舍。如果读操作也要求线性一致性（即必须读到最新的数据），那么所有读请求都得交给 Leader 处理，或者需要一个复杂的读协议，这样就无法通过增加服务器来扩展读性能。

ZooKeeper 的目标应用场景通常是“读多写少”。为了极大地提升读的吞吐量，ZooKeeper 允许每个服务器副本（Follower）直接用自己的本地内存数据库来响应读请求。但这样一来，副本的数据可能暂时落后于 Leader ，导致客户端可能会读到 **陈旧数据 (stale data)** 。

这听起来很危险，但 ZooKeeper 认为对于协调服务来说，这种“最终一致”的读是可以接受的。并且，它提供了 `sync()` 这个“后悔药”，如果某个读操作确实需要最新数据，可以在读之前调用一次 `sync()` 。`sync()` 会强制当前客户端连接的服务器与 Leader 同步，确保后续的读能看到最新的状态。

### 生产实践：用 ZooKeeper 搭建协调原语

有了 znode、watch 和强大的顺序保证，我们就可以构建各种上层应用了。

#### 动态配置管理

这是最简单的用法。将配置信息存放在一个 znode `/app/config` 中。所有应用进程启动时读取这个 znode 的数据，并设置一个 watch 。当配置需要变更时，管理员只需修改这个 znode 的内容。所有设置了 watch 的进程都会收到通知，然后重新读取配置，实现动态更新。

#### 服务发现与组成员管理

利用临时节点可以完美实现这个功能。假设有一个服务集群，每个服务实例启动时，都在一个公共的 znode `/service/members` 下创建一个代表自己的临时节点，比如 `/service/members/instance-1` 。节点的数据可以存放该实例的 IP 和端口。

* **成员发现** ：其他客户端只需 `getChildren("/service/members", watch=true)`，就能获取当前所有在线服务的列表。
* **故障检测** ：如果某个服务实例崩溃或网络断开，它与 ZooKeeper 的会话会超时，其对应的临时节点会被自动删除。监听 `/service/members` 的其他客户端会收到子节点变化的通知，从而知道有成员下线了。

#### 分布式锁（避免羊群效应）

一个简单的锁可以通过 `create("/lock", EPHEMERAL)` 来实现，谁创建成功谁就获得锁。但这会导致 **羊群效应 (herd effect)** ：一旦锁释放，所有等待的客户端会同时被唤醒，然后蜂拥而上尝试创建节点，造成瞬间的网络风暴，而最终只有一个能成功。

更优雅的做法是利用顺序节点：

**获取锁 (Acquire)**

1.  在锁目录 `/lock` 下，创建一个 **临时顺序节点** ，比如得到 `/lock/lock-0000000002` 。
2.  获取 `/lock` 下的所有子节点，并排序。
3.  判断自己创建的节点是不是序号最小的。如果是，则成功获得锁。
4.  如果不是，就找到比自己序号小一位的节点（比如 `lock-0000000001`），并对它设置 `exists(watch=true)` 。
5.  然后等待，直到收到 watch 通知。
6.  收到通知后，回到第 2 步，重新检查自己是不是最小的。

**释放锁 (Release)**

* 客户端完成任务后，只需删除自己创建的那个临时节点即可。如果客户端崩溃，节点也会自动删除。

这个方案中，锁的释放只会唤醒队列中的下一个等待者，完美避免了羊群效应。

#### 领导者选举

领导者选举和分布式锁非常相似，通常获胜的进程会把自己的信息写入一个约定的 znode，其他进程 watch 这个 znode 来感知 Leader 的变化和存活状态。

### 实际应用与常见问题

ZooKeeper 是许多著名开源项目的基石，比如：

* **Apache Kafka** ：用它来存储 Broker 和 Consumer 的元数据，进行领导者选举等。
* **Apache Hadoop/HDFS** ：用于 NameNode 的高可用方案，选举 Active NameNode。
* **Apache HBase** ：用于确保集群中只有一个 Master，并存储 Region Server 的状态。

**常见问题与解决方案：**

* **问：客户端断线重连到另一个服务器，会不会读到“倒退”的数据？**
    * 答：不会。客户端会话中会记录它所见过的最新事务 ID，即 **zxid** 。当它重连到一个新服务器时，新服务器会检查客户端的 zxid。如果服务器自己的状态比客户端的还旧，它会拒绝建立会话，直到它从 Leader 那里同步到足够新的状态为止。
* **问：如何处理“羊群效应”？**
    * 答：如上文所述，使用顺序节点和只 watch 前一个节点的策略来实现有序、无惊群的锁。
* **问：会话超时时间应该设多长？**
    * 答：这是一个权衡。太短，网络抖动可能导致节点被误判为“死亡”，造成服务频繁切换。太长，节点真的宕机后，系统需要更长时间才能发现并恢复。客户端库通常会在超时时间的 1/3 时发送心跳，在 2/3 时间内没收到响应时就尝试连接新服务器，以增加鲁棒性。
* **问：ZooKeeper 性能如何？**
    * 答：读性能极高，并且可以通过增加服务器数量来水平扩展。写性能会随着服务器增多而略有下降，因为 Leader 需要将写入请求同步给大多数 Follower 。但在现代硬件上，一个小型集群处理数万的写入 QPS 也是可能的。

总而言之，ZooKeeper 通过提供一个看似简单、实则经过深思熟虑的数据模型和 API，成功地将分布式协调中那些最棘手、最普遍的问题抽象出来，用一个可靠、高性能的“内核”予以解决。它让应用开发者可以更专注于业务逻辑，而不是陷入分布式共识的泥潭。

---

## 补充：ZooKeeper 核心实现原理以及一个系统与 ZooKeeper 的完整交互流程

接下来我们把问题拆开，一步步搞清楚 ZooKeeper 的“黑盒子”里到底是什么，以及它如何与外部系统配合。

### Part 1：ZooKeeper 核心实现原理

您可以将 ZooKeeper 集群想象成一个高度专业的“秘书处”。这个秘书处有多名秘书（服务器），但只有一个是“首席秘书”（Leader），其余都是“普通秘书”（Follower）。他们的工作是为各个部门（客户端应用）管理和同步一些至关重要的、但量不大的公共信息（例如“谁是当前的项目总负责人？”）。

其核心实现主要依赖以下几个组件：

#### 1. 复制状态机与内存数据库

* **核心数据结构** ：ZooKeeper 的所有数据都存储在一个内存中的树状结构里，这棵树由 **数据节点 (znodes)** 构成。因为数据在内存中，所以读操作非常快。
* **高可用性** ：为了保证高可用，这个内存数据库会在集群中的每一台服务器上都完整地复制一份。
* **持久化** ：为了防止服务器崩溃导致数据丢失，所有修改数据的操作都会以“事务日志”的形式预先写入磁盘（Write-Ahead Log），并且会定期生成数据库的快照（Snapshot）。

#### 2. Leader、Follower 与 Zab 协议

ZooKeeper 集群并非所有服务器都地位平等，它是一个基于 Leader 的系统。

* **Leader (领导者)**
    * 是整个集群的“事务处理中心”。
    * 所有“写”请求（创建、修改、删除 znode）都必须由 Leader 来处理。
    * 负责将数据变更同步给所有的 Follower。
* **Follower (跟随者)**
    * 可以独立处理“读”请求（获取 znode 数据、查询子节点等）。这使得 ZooKeeper 的读性能可以随着服务器数量的增加而水平扩展。
    * 参与 Leader 的选举。
    * 接收并确认来自 Leader 的写操作提案。

**Zab (ZooKeeper Atomic Broadcast) 协议**
  
  这是保证数据一致性的核心。您可以将它理解为一个加强版的“广播系统”，确保 Leader 发出的每一个“状态变更”指令都能被可靠地、严格按顺序地传递给所有 Follower 。一个写请求只有在被 **超过半数** 的服务器确认收到并写入日志后，Leader 才会真正“提交”这个修改，并对客户端做出响应。这保证了任何已提交的数据都不会因少数服务器的故障而丢失。

#### 3. 请求处理流程

理解了上述角色后，我们来看看一个请求是如何被处理的：

* **当一个“写”请求到达时**

1.  如果请求发给了 Follower，Follower 会把它转发给 Leader 。
2.  Leader 执行这个请求，生成一个代表状态变更的“事务”。
3.  Leader 通过 Zab 协议将这个事务广播给所有 Follower 。
4.  当超过半数的 Follower 确认收到并成功将事务写入本地磁盘日志后，Leader 就会提交该事务，并将变更应用到内存数据库中。
5.  最后，处理该请求的服务器（可能是 Leader 或最初接收请求的 Follower）向客户端发送成功响应。

* **当一个“读”请求到达时**

1.  无论是 Leader 还是 Follower，收到读请求后，会直接查询自己的本地内存数据库。
2.  立即将查询结果返回给客户端。这个过程不涉及服务器间的通信，因此速度极快。

**关键点** ：由于读请求由各服务器本地处理，而数据同步存在微小延迟，所以客户端从 Follower 读取到的数据可能是“轻微过时”的 (stale data) 。但在大多数协调场景下，这种短暂的不一致是可以接受的。

---

### Part 2：一个系统与 ZooKeeper 的完整交互流程

让我们以一个非常经典的场景为例： **一个高可用的 Web 服务，需要通过 Leader 选举来保证永远只有一个主服务器在工作** 。

假设我们有3台服务器：`Server-A`, `Server-B`, `Server-C`，它们都要运行这个 Web 服务，但同一时间只能有一个是 Active（Leader），其他都是 Standby（Follower）。

**交互流程如下：**

#### 第 1 步：初始化 (管理员操作)

管理员首先连接到 ZooKeeper 集群，创建一个用于选举的永久 znode 作为“锁目录”。

```bash
# 连接 ZooKeeper 命令行客户端
zkCli.sh -server <zookeeper_host>:<port>

# 创建一个永久的 znode
create /myapp/leader_election "election root"
```

#### 第 2 步：服务启动，参与选举 (所有服务器)

`Server-A`, `Server-B`, `Server-C` 依次启动。它们各自都会执行以下逻辑：

1.  **连接 ZooKeeper** ：每个服务与 ZooKeeper 集群建立一个会话 (Session)。

2.  **尝试创建节点** ：每个服务都尝试在 `/myapp/leader_election` 目录下创建一个 **临时 (EPHEMERAL)** 且 **顺序 (SEQUENTIAL)** 的 znode 。

* `Server-A` 创建成功，得到路径 `/myapp/leader_election/node-0000000001`。
* `Server-B` 创建成功，得到路径 `/myapp/leader_election/node-0000000002`。
* `Server-C` 创建成功，得到路径 `/myapp/leader_election/node-0000000003`。

**为何要用这两个标志？**
* `EPHEMERAL`：临时节点与会话绑定。如果一个服务器宕机，它与 ZooKeeper 的会话会超时，这个节点就会被 **自动删除** 。这天然地解决了故障检测和锁自动释放的问题。
* `SEQUENTIAL`：顺序节点保证了所有参与者的请求都有一个全局唯一的、递增的序号。这为决定谁是 Leader 提供了公平、无争议的依据。

#### 第 3 步：判断谁是 Leader (所有服务器)

创建完节点后，每个服务器都会执行以下判断逻辑：

1.  **获取子节点列表** ：调用 `getChildren("/myapp/leader_election")`，得到 `[node-0000000001, node-0000000002, node-0000000003]`。
2.  **判断自身排名** ：每个服务器检查自己创建的节点序号是否是最小的。
* `Server-A` 发现自己的 `node-0000000001` 序号最小，于是它 **成为 Leader** 。它开始对外提供服务，并定期向 ZooKeeper 发送心跳以维持会话。
* `Server-B` 和 `Server-C` 发现自己不是最小的，于是它们进入  **Standby 状态** 。

#### 第 4 步：Follower 等待 (Server-B, Server-C)

`Server-B` 和 `Server-C` 不能只是“干等”，它们需要一种高效的方式来知晓 Leader 何时“退位”。

1.  `Server-B` (拥有 `node-0000000002`) 计算出比自己序号小一位的节点是 `node-0000000001`。
2.  `Server-B` 调用 `exists("/myapp/leader_election/node-0000000001", watch=true)`，对当前 Leader 的节点设置一个 **watch** 。
3.  同样，`Server-C` 会对 `node-0000000002` (即 `Server-B` 的节点) 设置一个 watch。

它们现在就安静地等待 watch 通知，不进行任何轮询，资源开销极小。

#### 第 5 步：故障发生与自动切换

突然，`Server-A` 因硬件故障宕机了。

1.  `Server-A` 停止向 ZooKeeper 发送心跳。
2.  经过一个会话超时时间 (Session Timeout)，ZooKeeper 判定 `Server-A` 已“死亡”。
3.  ZooKeeper **自动删除了** `Server-A` 创建的临时节点 `/myapp/leader_election/node-0000000001` 。

#### 第 6 步：新 Leader 诞生

1.  `node-0000000001` 的删除事件，**触发了 `Server-B` 设置的 watch** 。
2.  `Server-B` 被唤醒，它重新执行 **第 3 步** 的逻辑：获取子节点列表（现在是 `[node-0000000002, node-0000000003]`），发现自己创建的 `node-0000000002` 序号已经是最小的了。
3.  `Server-B` **立即切换到 Active 状态，成为新的 Leader** ，开始对外提供服务。
4.  与此同时，`Server-C` 仍然在 watch `node-0000000002`，继续作为 Standby 等待。

至此，系统就完成了一次无人工干预的、平滑的 Leader 切换。

### 总结

* **ZooKeeper 是什么？**
  * 它是一个高可用的 **协调服务** 。它本身不存储业务数据，而是像一个共享的、可靠的公告板，用来维护和同步分布式系统中至关重要的 **元数据** （如配置、状态、成员列表等）。
* **它如何发挥作用？**
  * 它通过提供一套简单但强大的原语（如临时顺序节点和 watch 机制），以及可靠的一致性保证（Zab 协议），让复杂的分布式协调逻辑（如 Leader 选举、分布式锁、服务发现）的实现变得 **简单和可靠** 。应用程序通过与 ZooKeeper 交互，读写这些简单的 znode，就能实现自身复杂的分布式行为。
