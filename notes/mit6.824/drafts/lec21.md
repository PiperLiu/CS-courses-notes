# LEC 21: Project demos

<!-- @import "[TOC]" {cmd="toc" depthFrom=2 depthTo=6 orderedList=false} -->

<!-- code_chunk_output -->

- [AnalogicFS](#analogicfs)
- [来看看两次考试吧](#来看看两次考试吧)
  - [MIT 6.824 2020 年期中考试解析](#mit-6824-2020-年期中考试解析)
    - [MapReduce (1): 如何用 MapReduce 找最大值？](#mapreduce-1-如何用-mapreduce-找最大值)
    - [MapReduce (2): 为什么写入中间文件需要“先写临时文件再重命名”？](#mapreduce-2-为什么写入中间文件需要先写临时文件再重命名)
    - [GFS: 同时读同一个 GFS 文件，内容一定相同吗？](#gfs-同时读同一个-gfs-文件内容一定相同吗)
    - [Raft (1): `currentTerm` 必须持久化吗？](#raft-1-currentterm-必须持久化吗)
    - [Raft (2): `AppendEntries` 时直接覆盖日志行不行？](#raft-2-appendentries-时直接覆盖日志行不行)
    - [ZooKeeper: 为什么读到旧数据也不会错误地拿到锁？](#zookeeper-为什么读到旧数据也不会错误地拿到锁)
    - [CRAQ: 为什么不能直接返回“脏”数据？](#craq-为什么不能直接返回脏数据)
    - [Frangipani: 如何保证缓存一致性？](#frangipani-如何保证缓存一致性)
  - [MIT 6.824 2020 年期末考试解析](#mit-6824-2020-年期末考试解析)
    - [事务与可串行化 (Transactions and Serializability)](#事务与可串行化-transactions-and-serializability)
    - [两阶段提交 (Two-Phase Commit)](#两阶段提交-two-phase-commit)
    - [Spanner: 为什么所有写操作要用同一个时间戳？](#spanner-为什么所有写操作要用同一个时间戳)
    - [Spark: `for` 循环为什么执行那么快？](#spark-for-循环为什么执行那么快)
    - [FaRM (1): 两个相同事务同时执行会怎样？](#farm-1-两个相同事务同时执行会怎样)
    - [FaRM (2): 把 `VALIDATE` 和 `LOCK` 顺序颠倒会怎样？](#farm-2-把-validate-和-lock-顺序颠倒会怎样)
    - [Spark: 为什么在循环里 `persist()` RDD 没用？](#spark-为什么在循环里-persist-rdd-没用)
    - [Memcache: 为什么简单的“写数据库再写缓存”会出错？](#memcache-为什么简单的写数据库再写缓存会出错)
    - [COPS: 因果一致性下的读操作](#cops-因果一致性下的读操作)
    - [Lab 3 Raft (1): 为什么 Follower 不能直接响应 `Get` 请求？](#lab-3-raft-1-为什么-follower-不能直接响应-get-请求)
    - [Lab 3 Raft (2): 如何用 TrueTime 让 Follower 安全地响应 `Get` ？](#lab-3-raft-2-如何用-truetime-让-follower-安全地响应-get-)

<!-- /code_chunk_output -->

## AnalogicFS

AnalogicFS 是一篇由机器生成的“恶作剧”论文。

## 来看看两次考试吧

### MIT 6.824 2020 年期中考试解析

#### MapReduce (1): 如何用 MapReduce 找最大值？

这道题非常经典，是理解 MapReduce 思想的敲门砖。

**问题背景** ：想象一下，我们有 100 个文件，每个文件里都写满了数字，一行一个。我们的任务是找出这所有数字里的最大值。

**解读**

MapReduce 的核心思想是“分而治之”。

1.  **`map` 阶段** ：`map` 函数就像是“地方海选赛”。每个 `map` 任务会分配到一个或多个文件。它的工作很简单：就在自己负责的这堆数里，找出那个最大的“地方冠军”。然后，它会输出一个键值对，比如 `( "max", 12345 )`，其中 `12345` 就是它找到的那个局部最大值。这里用一个固定的 `key` (比如空字符串 `""` 或者 `"max"`) 是个小技巧，目的是确保所有这些“地方冠军”都能被送到同一个 `reduce` 任务那里去进行“总决赛”。

2.  **`reduce` 阶段** ：`reduce` 函数就是“全国总决赛”。因为前面所有的 `map` 任务都用了同一个 `key`，所以 MapReduce 框架会把所有 `map` 的输出（也就是所有“地方冠军”的数值）都集合起来，然后交给一个 `reduce` 任务。这个 `reduce` 任务的工作就更简单了：在这些“地方冠军”里，选出那个唯一的“全国总冠军”，也就是全局最大值。

**`reduce` 函数会被调用几次？**

只会有 1 次。因为我们巧妙地设计了让所有 `map` 的输出都使用同一个 `key`，所以这些数据只会被汇集到一个 `reduce` 任务中处理。

#### MapReduce (2): 为什么写入中间文件需要“先写临时文件再重命名”？

**问题背景** ：有个同学叫 Alyssa，她在实现 MapReduce 的 `worker` 时偷懒了。她直接用 `os.Create()` 来创建中间结果文件，而不是遵循“先写到一个临时文件，写完后再用 `os.Rename()` 重命名”这个最佳实践。这会出什么问题？

**解读**

这个问题触及了分布式系统中一个非常重要的概念：处理“慢节点”和任务的原子性。

在 MapReduce 中，`master` 有一个叫做 投机执行 (speculative execution) 的机制。如果它发现某个 `map` 任务运行得特别慢，它可能会在另一台机器上重新启动一个一模一样的任务，谁先跑完就用谁的结果。

现在，我们来想象一个灾难场景：

1.  `master` 派任务 `M` 给 `worker W1`。`W1` 由于网络、CPU 等原因，运行得非常慢。
2.  `master` 等得不耐烦了，启动了投机执行，把同样的任务 `M` 又派给了 `worker W2`。`W2` 身强力壮，很快就完成了计算，并用 `os.Create("mr-M-R")` 创建并写好了中间文件。
3.  `master` 收到 `W2` 的捷报，于是启动了对应的 `reduce` 任务，这个 `reduce` 任务开始读取 `W2` 生成的那个 `mr-M-R` 文件。
4.  就在此时，慢吞吞的 `W1` 终于也完成了它的计算。它也执行了 `os.Create("mr-M-R")`。
5.  关键点来了：`os.Create()` 在文件已存在时，会直接清空它！于是，`W2` 辛辛苦苦生成的、`reduce` 任务正在读取的文件，瞬间被 `W1` 清空了。
6.  `reduce` 任务读着读着发现文件变空了，最终得出了错误的结果。

**正确的做法是什么呢？**

原子性的“写入临时文件后重命名”。`W1` 和 `W2` 都先写入各自的临时文件（比如 `mr-M-R-temp-W1`），写完后，再用 `os.Rename()` 这个原子操作去抢占最终的文件名。这样就能保证，无论谁快谁慢，`reduce` 任务读到的文件一定是某个 `worker` 完整写入的结果，而不是一个被中途清空的文件。

#### GFS: 同时读同一个 GFS 文件，内容一定相同吗？

**问题背景** ：两个客户端，在没有任何写入操作的情况下，同时从头到尾读取 GFS 上的同一个文件。它们读到的内容保证会一样吗？

**解读**

不保证。 GFS 在设计上为了性能和可用性，在某些一致性上做了妥协。

问题的根源在于 GFS 的一种特殊写操作：记录追加 (record append)。当一个客户端执行追加操作时，`primary` 副本会确定一个偏移量，然后通知所有 `secondary` 副本也写入。但如果某个 `secondary` 副本当时正好网络不通或者挂了，它可能就收不到这个写入指令。GFS 的 `primary` 不会死等所有副本都成功，它只会把错误报告给客户端。

这就导致了一个后果：同一个数据块的不同副本（chunk replica），可能内容不一样了。一个副本有这次追加的数据，另一个没有。

所以，当那两个客户端来读取文件时，如果它们不幸地连接到了持有不同数据副本的 `chunkserver` 上，它们读到的内容自然也就不一样了。

#### Raft (1): `currentTerm` 必须持久化吗？

**问题背景** ：Ben 同学觉得每次都持久化 `currentTerm` 太麻烦，他想了个“聪明”的办法：当一个节点重启时，不从持久化存储里读 `currentTerm`，而是直接读取它日志里最后一条记录的任期号，并把它作为自己的 `currentTerm`。这会出什么问题？

**解读**

Ben 的这个改动会破坏 Raft 协议的根基——投票的正确性，从而可能导致“脑裂”（即同一任期出现两个 `leader`）。

`currentTerm` 和 `votedFor` 这两个状态，是 Raft 节点在选举中的“记忆”。它们必须被持久化，以确保节点在崩溃重启后不会“失忆”并做出矛盾的决定。

我们来看一个具体的失败场景：

1.  一个集群，节点 `P1` 的日志里最后一条记录的任期是 10。所以它当前的 `currentTerm` 也是 10。
2.  候选人 `P2` 发起了 term 11 的选举。`P1` 收到投票请求，它一看任期比自己的高，于是投票给了 `P2`。同时，`P1` 把自己的（内存中的）`currentTerm` 更新为 11，并持久化 `votedFor = P2`。
3.  在 `P1` 还来不及持久化 `currentTerm = 11` 的时候，它突然崩溃了。
4.  `P1` 重启。按照 Ben 的逻辑，它会读取日志，发现最后一条记录的任期是 10，于是它把自己的 `currentTerm` 初始化为 10。
5.  这时，另一个候选人 `P3` 也发起了 term 11 的选举。`P3` 的投票请求到达了 `P1`。`P1` 检查后发现，请求的任期 11 比自己的当前任期 10 要高，并且（假设它没持久化 `votedFor` 或者 `votedFor` 逻辑也有问题）它认为自己还没在 term 11 里投过票。于是，它又投票给了 `P3`！

**灾难发生了** ：`P1` 在同一个任期 11 里，先后为 `P2` 和 `P3` 两个不同的候选人投了票。这严重违反了 Raft 的选举安全规则，完全可能导致 `P2` 和 `P3` 都分别获得足够选票成为 `leader`，系统出现“双主”，状态机将执行不同的指令，数据一致性被破坏。

#### Raft (2): `AppendEntries` 时直接覆盖日志行不行？

**问题背景** ：Bob 同学为了简化代码，修改了 `AppendEntries` RPC 的处理逻辑。他不再检查日志冲突，而是简单粗暴地直接用 `leader` 发来的日志覆盖本地日志。这为什么是错的？

**解读**

这个改动破坏了 Raft 的日志匹配属性 (Log Matching Property)，这是确保安全性的核心。直接覆盖会导致一个已提交的日志条目被错误地更改。

看这个例子：

1.  有三个节点 `S1`, `S2`, `S3`。`S1` 是 term 1 的 `leader`。
2.  `S1` 在 index 1 追加了日志 `A`，在 index 2 追加了日志 `B`。它把 `[A, B]` 发给了 `S2` 和 `S3`。
3.  `S2` 成功收到了 `[A, B]`。`S1` 和 `S2` 构成了多数派，所以 `A` 和 `B` 在 `S1` 上被提交了。`S3` 可能因为网络延迟只收到了 `A`。
4.  现在，一个 **之前** 从 `S1` 发出的、但被网络延迟了的 `AppendEntries` RPC（这个 RPC 只包含 `A`）终于到达了 `S2`。
5.  按照 Bob 的错误逻辑，`S2` 不做冲突检查，直接用这个 RPC 的内容来更新自己的日志。它会把自己的日志从 `[A, B]` 截断回 `[A]`。
6.  `S1` 挂了。`S3` 发起 term 2 的选举。`S3` 的日志是 `[A]`，`S2` 的日志现在也是 `[A]`，所以 `S2` 会投票给 `S3`。
7.  `S3` 成为 term 2 的新 `leader`。它在 index 2 写入了一个 **不同** 的日志 `C`。
8.  `S3` 把 `C` 复制给了 `S2`，并且它们俩构成了多数派，提交了 `C`。

**最终结果** ：在 index 2 这个位置，`S1` 提交的日志是 `B`，而 `S2` 和 `S3` 提交的却是 `C`。不同的节点在同一个日志索引上提交了不同的命令，状态机不再一致，Raft 的安全性被彻底打破。

#### ZooKeeper: 为什么读到旧数据也不会错误地拿到锁？

**问题背景** ：我们知道 ZooKeeper 的读操作在某些情况下可能会返回“陈旧”的数据。在使用 ZooKeeper 实现分布式锁时，客户端 `C2` 会调用 `getChildren()` 来检查自己创建的节点是不是序号最小的那个。那么，有没有可能因为 `getChildren()` 读到了旧数据，没看到当前锁持有者 `C1` 的节点，从而错误地以为自己获得了锁呢？答案是：不可能。为什么？

**解读**

这个问题的关键在于 ZooKeeper 提供的一个重要保证，叫做 **客户端顺序保证 (client order guarantee)** 。

虽然一个客户端的读操作，可能读到比系统中最新状态要旧的数据，但 ZooKeeper 承诺，对于 **同一个客户端** 的会话，它的操作是按顺序执行的。也就是说，如果你先执行了一个写操作，那么你后续的读操作，一定能看到这个写操作（或更晚的写操作）之后的状态。

我们来分析一下获取锁的流程：

1.  客户端 `C2` 想要获取锁，它首先会执行一个 `create()` 操作，在锁目录下创建一个代表自己的、带序列号的临时节点，比如 `/lock/node-002`。
2.  然后，`C2` 在同一个会话中，执行 `getChildren()` 操作，获取锁目录下的所有节点列表。
3.  根据“客户端顺序保证”，`C2` 的这次 `getChildren()` 操作，看到的数据状态 **至少** 和它自己刚刚 `create()` 操作成功后的状态一样新。
4.  而当前的锁持有者 `C1`，它创建自己的锁节点 `/lock/node-001` 的时间点，必然比 `C2` 创建 `/lock/node-002` 更早。
5.  因此，`C2` 的 `getChildren()` 看到的视图，既然新到足以包含自己刚刚创建的 `/lock/node-002`，那么它也 **必然** 会包含更早之前就已存在的 `/lock/node-001` 。

所以，`C2` 绝不会“看不见” `C1` 的节点，也就不会错误地认为自己是序号最小的那个。

#### CRAQ: 为什么不能直接返回“脏”数据？

**问题背景** ：CRAQ (Chain Replication with Apportioned Queries) 是一个链式复制系统。当一个写请求到达链头，数据在被最终提交（即到达链尾）之前，在链头和中间节点上处于“脏” (dirty) 状态。CRAQ 的论文规定，如果一个节点收到了对脏数据的读请求，它应该去问链尾，获取最新的已提交版本号。如果我们修改这个规则，允许节点直接返回它本地最新的、即便是“脏”的版本，会如何破坏系统的 **强一致性** (strong consistency)？

**解读**

直接返回脏数据会破坏系统的 **线性一致性 (linearizability)** ，这是强一致性的一种具体体现。线性一致性要求，所有操作看起来都像是在某个单一的时间点上以原子的方式瞬间完成的。一个重要的推论是：如果操作 A 在操作 B 开始前就已经完成，那么 B 必须能看到 A 操作的结果。

我们来看一个具体的失败场景：

1.  假设链上有三个服务器 `S1 (链头) -> S2 -> S3 (链尾)`。变量 `X` 的初始值为 1。
2.  客户端 `C1` 发起一个写操作，要将 `X` 设置为 2。这个写请求到达了链头 `S1`。现在，`S1` 上 `X` 的值是 2（脏数据），而 `S2` 和 `S3` 上 `X` 的值仍然是 1（已提交的旧数据）。
3.  客户端 `C2` 向 `S1` 发起读请求。按照我们修改后的错误逻辑，`S1` 直接返回了脏数据 2 。
4.  `C2` 收到了 2 这个值，这个读操作完成了。 **紧接着** ，`C2` 又向链尾 `S3` 发起了对 `X` 的读请求。
5.  `S3` 上只有已提交的数据，所以它返回了 1 。
6.  现在，从 `C2` 的视角来看，它先读到了 `X=2`，后读到了 `X=1`。它观察到系统的状态从 2 变回了 1，仿佛时间倒流了。这显然无法用任何一个统一的、串行的操作历史来解释，因此线性一致性被打破。

#### Frangipani: 如何保证缓存一致性？

**问题背景** ：Frangipani 是一个分布式文件系统。同事 Aviva 和 Chetty 都在用它。Chetty 读取了一个文件，文件内容被缓存在她的本地工作站。然后 Aviva 修改了同一个文件，并朝 Chetty 喊了一声“我改完啦！”。当 Chetty 再次读取文件时，Frangipani 如何保证她能看到 Aviva 的最新修改，而不是读到自己缓存里的旧数据？

**解读**

这背后是一套由 **分布式锁服务** 驱动的、优雅的缓存一致性协议。

整个过程像一场精心编排的舞蹈：

1.  **Aviva 请求写入** ：当 Aviva 的工作站 (我们称之为 `WA`) 想要修改文件时，它不能直接动手。它必须先向中央锁服务请求获得该文件的 **独占写锁** (exclusive write lock)。
2.  **锁服务协调** ：锁服务发现 Chetty 的工作站 (`WC`) 正持有该文件的共享读锁。为了把独占锁授予 `WA`，锁服务会向 `WC` 发送一个 **锁吊销** (lock revocation) 请求。
3.  **Chetty 清理缓存 (关键步骤!)** ：`WC` 收到锁吊销请求后，在它真正释放读锁之前，它被协议 **强制要求** 必须清理掉自己本地缓存里所有受该锁保护的数据。因此，Chetty 缓存的旧文件内容就在这一刻被删除了。
4.  **Aviva 完成修改** ：`WC` 释放锁后，`WA` 成功获得了独占写锁，现在它可以安全地修改文件了。修改完成后，当 `WA` 释放锁时，会将更新的内容写入后端的共享存储系统 Petal 。
5.  **Chetty 再次读取** ：当 Chetty 在 Aviva 喊完后再次读取文件时，它的工作站 `WC` 会发现本地缓存是空的（因为之前被清掉了）。`WC` 就会像第一次读一样，先去获取一个读锁，然后从共享存储 Petal 中读取文件内容。此时，它读到的自然就是 Aviva 写入的最新版本了。

### MIT 6.824 2020 年期末考试解析

#### 事务与可串行化 (Transactions and Serializability)

**问题背景** ：有三个并发事务 T1, T2, T3。初始时，数据库里的变量 `x`, `y`, `z` 都是 0。

```txt
T1:       T2:         T3:
begin()   begin()     begin()
put(y, 2) put(x, 99)  tmpx = get(x)
end()     put(y, 99)  tmpy = get(y)
          put(z, 99)  tmpz = get(z)
          end()       print tmpx, tmpy, tmpz
                      end()
```

**问题 1：如果 T3 打印出 `99, 2, 99`，这个结果是可串行化的吗？**

**解读**

是的，这是可串行化的。

可串行化 (Serializable) 的意思是，尽管事务是并发执行的，但其最终结果必须等同于这些事务按照 **某一个** 串行顺序执行的结果。我们的任务就是去找到这个串行顺序。

我们来试试 `T2 -> T1 -> T3` 这个顺序：

1.  先执行 `T2`：`x` 变成 99，`y` 变成 99，`z` 变成 99。
2.  接着执行 `T1`：`y` 被更新为 2。现在状态是 `x=99, y=2, z=99`。
3.  最后执行 `T3`：读取 `x` 得到 99，读取 `y` 得到 2，读取 `z` 得到 99。打印结果 `99, 2, 99`。

完全匹配！既然我们找到了一个能产生同样结果的串行顺序，那么这个结果就是可串行化的。

**问题 2：如果 T3 打印出 `0, 2, 99`，这个结果是可串行化的吗？**

**解读**

不，这不是可串行化的。

这次我们无法找到任何一个合法的串行执行顺序。我们可以用依赖关系来分析：

* `T3` 读到了 `x = 0`。而 `T2` 会把 `x` 改成 99。为了能读到 0，`T3` 的 `get(x)` 必须发生在 `T2` 的 `put(x, 99)` 之前。所以，在任何等价的串行顺序中，必然有 `T3` 在 `T2` 之前。
* `T3` 读到了 `z = 99`。`z` 的初始值是 0，只有 `T2` 会把它改成 99。为了能读到 99，`T3` 的 `get(z)` 必须发生在 `T2` 的 `put(z, 99)` 之后。所以，在任何等价的串行顺序中，必然有 `T2` 在 `T3` 之前。

这里就出现了致命的矛盾：`T3` 必须在 `T2` 之前，同时 `T2` 又必须在 `T3` 之前。这是不可能的。这种依赖环路意味着不存在任何一个串行顺序能产生这个结果，因此它不是可串行化的。

#### 两阶段提交 (Two-Phase Commit)

**问题背景** ：在两阶段提交 (Two-Phase Commit, 2PC) 协议中，`worker` 在投票 `PREPARE` 成功后，需要一直持有锁，直到收到最终的 `COMMIT` 或 `ABORT` 消息。如果我们改动一下，让 `worker` 在回复 `PREPARE` 后就立即释放锁，会发生什么？

**解读**

这么做会彻底破坏事务的原子性和隔离性。`PREPARE` 阶段结束后，`worker` 处于一个“不确定”的状态，它并不知道事务最终是会成功还是失败。在这个节骨眼上释放锁，会引发两种严重的问题：

1.  **读到“脏数据” (Dirty Reads)** ：如果 `worker` 释放了锁，并且让其他事务能够看到它本地“预提交”的修改（比如 `T1` 修改了 `x` 的值）。此时，另一个事务 `T_other` 进来读到了这个新值。但万一 `T1` 的协调者最终决定 `ABORT` 整个事务，那么 `T_other` 就相当于读到了一个从未真实存在过的数据，后续的所有计算都是基于这个“幻影”数据，后果不堪设想。
2.  **破坏可串行化** ：另一种情况是，`worker` 释放了锁，但很“聪明”地把修改先隐藏起来，不让别的事务看见。但即使这样，另一个事务 `T_other` 还是可以进来获取 `T1` 刚刚释放的锁。`T_other` 可能会读取一些 `T1` 没有修改过的数据（这是旧值），然后 `T1` 的 `COMMIT` 消息到达，`T1` 的修改被应用。之后，`T_other` 又读取了 `T1` 修改过的数据（这是新值）。这样一来，`T_other` 在一个事务里，既看到了过去，又看到了未来，看到了一个数据不一致的“混合快照”，这同样破坏了可串行化。

**结论** ：锁必须持有到事务的最终状态（`COMMIT` 或 `ABORT`）被确定为止，这是 2PC 保证隔离性的关键。

#### Spanner: 为什么所有写操作要用同一个时间戳？

**问题背景** ：在 Spanner 中，一个读写事务里的所有写操作，都会被赋予一个相同的提交时间戳。如果我们把它改成：每次客户端调用写操作时，就用当时的 `TT.now().latest` 作为这个写操作的时间戳。这样，一个事务内的不同写操作就会有不同的时间戳。这会破坏什么？

**解读**

这会破坏只读事务的可串行化保证。

Spanner 的一个核心特性是它能提供严格可串行化的只读事务。它通过给只读事务选择一个时间戳 `s_read`，然后读取在 `s_read` 时刻的数据库快照来实现的。

在原版 Spanner 中，一个读写事务 `T_rw` 的所有写操作共享一个提交时间戳 `s_write`。这样一来，对于任何只读事务，要么它的 `s_read` < `s_write`（完全看不到 `T_rw` 的修改），要么 `s_read` > `s_write`（能看到 `T_rw` 所有的修改）。这保证了原子性，`T_rw` 的修改对于只读事务来说是“要么全有，要么全无”的。

但如果按照问题中的修改，`T_rw` 的多个写操作 `W1, W2, W3` 会有各自不同的时间戳 `ts1, ts2, ts3`。这时，一个只读事务的时间戳 `s_read` 就可能恰好落在这些写操作之间，比如 `ts1 < s_read < ts2`。这意味着这个只读事务会看到 `W1` 的修改，但看不到 `W2` 和 `W3` 的修改。它看到了一个“半成品”状态的 `T_rw`，事务的原子性被打破，自然也就不再是可串行化的了。

#### Spark: `for` 循环为什么执行那么快？

**问题背景** ：Ben 在用 Spark 跑 PageRank，他发现代码里的那个 `for` 循环，每次迭代都只花几毫秒，整个循环跑完不到一秒。但整个 Spark 作业却要跑好几个小时。这是为什么？

**解读**

这是因为 Spark 的惰性求值 (lazy evaluation) 机制。

在 Spark 中，操作被分为两类：

* **转换 (Transformation)** ：比如 `map`, `filter`, `join` 等。这些操作并不会立即执行计算。它们只是在构建一个叫做 有向无环图 (DAG) 的计算蓝图。你可以想象成你在画一张建筑图纸，而不是真的在盖房子。
* **动作 (Action)** ：比如 `count`, `collect`, `saveAsTextFile` 等。只有当一个 `action` 被调用时，Spark 才会根据之前构建好的 DAG 图，真正地开始分发任务、读取数据、执行计算。

PageRank 的那个 `for` 循环里，全都是 `transformation` 操作，比如 `join` 和 `map`。每次循环，Spark 只是在图纸上又加了几笔，扩展了一下 DAG。这个过程当然非常快，因为没有涉及任何大规模的数据计算。真正耗时的计算，是在循环结束后，当某个 `action`（比如 `collect()` 或者把结果写入文件）被调用时，才一次性触发的。那几个小时，就是花在了执行这张庞大的计算图纸上。

#### FaRM (1): 两个相同事务同时执行会怎样？

**问题背景** ：在 FaRM 系统上，两个客户端在两台不同的机器上，同时发起一个完全相同的事务。初始时 `x=0, y=0` 。

```go
begin()
if x > y:
  y = y + 1
else:
  x = x + 1
end()
```

最终结果会是什么？FaRM 的提交协议是如何导致这个结果的？

**解读**

最终结果是： **一个事务提交，另一个事务中止** 。数据库的最终状态将是 `x=1, y=0` 。

这是由 FaRM 的乐观并发控制协议决定的：

1.  **读取阶段** ：两个事务（我们叫 `T_A` 和 `T_B`）同时开始。它们都读取到 `x=0` 和 `y=0` 。
2.  **逻辑判断** ：因为 `x > y` (`0 > 0`) 为假，所以两个事务都进入了 `else` 分支，都打算将 `x` 的值加 1 。
3.  **锁定阶段 (LOCK)** ：事务准备提交时，它们都需要为自己将要写入的对象请求锁。在这里，`T_A` 和 `T_B` 都会向负责对象 `x` 的服务器发送 `LOCK` 请求。
4.  **锁竞争** ：管理 `x` 的服务器几乎同时收到了两个 `LOCK` 请求。它会以一个确定的顺序来处理它们（比如按网络包到达的先后顺序）。假设它先处理了 `T_A` 的请求。
5.  **一成一败** ：`T_A` 成功获得了 `x` 的锁。接着，当服务器处理 `T_B` 的请求时，它发现 `x` 已经被锁定了，于是它会拒绝 `T_B` 的请求，并返回一个失败消息。
6.  **最终结局** ：`T_A` 顺利完成提交，将 `x` 的值更新为 1。而 `T_B` 因为锁请求失败，整个事务被中止。所以最后的结果是 `x=1, y=0`。

#### FaRM (2): 把 `VALIDATE` 和 `LOCK` 顺序颠倒会怎样？

**问题背景** ：Ben 同学觉得如果把 FaRM 提交协议中的 `VALIDATE` 阶段放到 `LOCK` 阶段之前，也许能让事务跑得更快。事实证明，这会破坏可串行化。请给出一个反例。

**解读**

这个改动会引入一种叫做 **写偏斜 (write skew)** 的异常。`VALIDATE` 的作用是检查你读取的数据在你执行事务期间是否被别人修改了。把它提前，你就无法发现那些在你验证之后、加锁之前发生的并发修改。

来看这个经典的转账/约束检查反例：

```txt
// 初始状态: x = 0, y = 0
T1:              T2:
if x == 0:       if y == 0:
  y = 1            x = 1
```

我们来模拟一下 Ben 修改后的错误流程：

1.  **读取** ：`T1` 和 `T2` 同时开始。`T1` 读取 `x=0`，`T2` 读取 `y=0`。
2.  **`VALIDATE` 阶段 (新)** ：`T1` 和 `T2` 同时发起验证。`T1` 验证 `x` 的版本号没变（仍然是 0），成功！`T2` 验证 `y` 的版本号没变（仍然是 0），也成功！
3.  **`LOCK` 阶段 (新)** ：现在两个事务都认为自己可以安全提交了。`T1` 请求 `y` 的锁，`T2` 请求 `x` 的锁。因为它们锁定的是不同的对象，所以两个请求都成功了！
4.  **提交** ：两个事务都提交了。`T1` 将 `y` 写入 1，`T2` 将 `x` 写入 1。最终数据库状态变为 `x=1, y=1` 。

**为什么这是错的？** 我们来看一下正确的串行执行结果：
* **`T1 -> T2`** : `T1` 执行，`x==0` 成立，`y` 变为 1。然后 `T2` 执行，`y==0` 不成立，什么也不做。最终结果：`x=0, y=1`。
* **`T2 -> T1`** : `T2` 执行，`y==0` 成立，`x` 变为 1。然后 `T1` 执行，`x==0` 不成立，什么也不做。最终结果：`x=1, y=0`。

可见，`x=1, y=1` 这个结果在任何串行顺序下都是不可能出现的。因此，Ben 的改动破坏了可串行化。

#### Spark: 为什么在循环里 `persist()` RDD 没用？

**问题背景** ：在 Spark PageRank 的迭代代码中，`ranks` RDD 在循环中被反复使用。Ben 认为在每次给 `ranks` 赋值后都调用 `ranks.persist()` 来缓存它，可以加速计算。但他发现，加了之后运行时间没变化。为什么？

**解读**

原因是 **RDD 的不可变性 (immutability)** 。

在 Spark 的 `for` 循环里，每一次迭代都会根据前一次迭代的 `ranks` RDD，通过一系列转换 (`join`, `map` 等)，生成一个 **全新的** `ranks` RDD。

让我们把循环展开看：
* 迭代 `i`：`ranks_i = ranks_{i-1}.join(...).map(...)`
* 迭代 `i+1`：`ranks_{i+1} = ranks_i.join(...).map(...)`

当 Ben 在第 `i` 次迭代的末尾调用 `ranks_i.persist()` 时，他确实把 `ranks_i` 这个 RDD 标记为需要缓存。然而，这个 `ranks_i` RDD 的生命周期里，它只被 **使用了一次** ——就是在第 `i+1` 次迭代中，作为计算 `ranks_{i+1}` 的输入。

缓存一个只用一次的东西是毫无意义的。这就好比你为了明天早上喝一杯牛奶，今天就把牛奶倒在杯子里放进冰箱，这没有任何加速作用。`persist()` 的真正威力在于，当你需要对 **同一个** RDD 执行 **多个** `action` 操作时，缓存它才能避免重复计算。

#### Memcache: 为什么简单的“写数据库再写缓存”会出错？

**问题背景** ：Ben 设计了一个简单的缓存系统：写操作总是先更新 MySQL 数据库，再更新 Memcached 缓存。他认为，这样只有在写数据库和写缓存之间的那个短暂瞬间，数据会不一致。为什么他错了？

**解读**

Ben 的错误在于，他忽略了 **并发** 和 **网络延迟** 这两个分布式系统中的“幽灵”。他想当然地认为，操作的顺序会和他代码里写的顺序一样，但这在分布式环境中完全不是一回事。

问题出在 **竞态条件 (race condition)** 上。我们想象一个场景：

1.  两个客户端 `C1` 和 `C2`，几乎在同一时间，都要对同一个键 `K` 进行写入。`C1` 想写 `val1`，`C2` 想写 `val2`。
2.  **在数据库端** ：由于网络路径和服务器处理时间的细微差别，`C1` 的更新请求先到达数据库，然后是 `C2` 的。数据库先将 `K` 更新为 `val1`，再更新为 `val2`。最终，数据库里存的是 `val2` 。这是正确的。
3.  **在 Memcached 端** ：同样由于网络延迟的随机性，这次 `C2` 的更新请求先到达了 Memcached，`K` 的值被设为 `val2`。紧接着，`C1` 的请求到达了，又将 `K` 的值覆盖为了 `val1` 。

**灾难性的后果** ：现在，数据库里 `K` 的值是 `val2`，而缓存里 `K` 的值是 `val1`。两者出现了永久性的不一致！这种不一致会一直持续下去，直到下一次对 `K` 的写入发生，这期间所有读缓存的请求都会得到错误的数据。

#### COPS: 因果一致性下的读操作

**问题背景** ：在 COPS 系统（非事务性的 COPS-GT）中，有四个客户端在不同数据中心并发执行操作，初始值 `x,y,z` 均为 0 。

```txt
C1: put(x, 1); put(y, 2)
C2: tmp_y = get(y); put(z, 100 + tmp_y)
C3: get(x); get(y); get(z); print ...  // 读序 x, y, z
C4: get(z); get(y); get(x); print ...  // 读序 z, y, x
```

**C3 可能的输出是什么？**

**解读**

```txt
[ ] x=1 y=2 z=102
[ ] x=1 y=0 z=102
[ ] x=0 y=2 z=102
[ ] x=1 y=2 z=0
[ ] x=1 y=0 z=0
[ ] x=0 y=2 z=0
```

答案是所有六个选项都有可能。

这说明，在非事务性的 COPS 中，单个的 `get` 操作之间没有同步。`C3` 连续执行三个 `get`，但每一个 `get` 都可能从本地数据中心的存储中读到不同“时间点”的数据。由于网络复制的延迟，`C3` 的本地数据中心可能只看到了 `C1` 的部分写入，或者看到了 `C2` 的写入但 `C1` 的写入还没到。因此，各种看似奇怪的组合都可能出现。

**C4 可能的输出是什么？**

**解读**

答案是只有 `x=1 y=2 z=102`, `x=1 y=2 z=0`, `x=1 y=0 z=0` 这三个。

为什么 `C4` 的可能性就变少了？关键在于 **读的顺序** 和 COPS 的 **因果一致性 (causal consistency)** 保证。

我们先梳理一下因果链：
* `C1` 的 `put(x,1)` **因果上先于** `put(y,2)`。
* 如果 `C2` 读到了 `y=2`，那么 `C2` 的 `put(z,102)` 就 **因果上依赖于** `C1` 的 `put(y,2)`。
* 所以，我们有一个潜在的因果长链：`put(x,1) -> put(y,2) -> put(z,102)`。

COPS 的核心保证是：如果你看到了一个结果，你必须能看到导致这个结果的所有原因。

现在来看 `C4` 的 `get(z); get(y); get(x);` 流程：

**情况一：`get(z)` 返回 102**

`C4` 看到了 `z=102` 这个结果。根据 COPS 的保证，它所在的本地数据中心必须也已经稳定地接收到了导致 `z=102` 的所有原因，也就是 `put(y,2)` 和 `put(x,1)`。因此，`C4` 接下来执行 `get(y)` 和 `get(x)` 时， **必然** 会读到 2 和 1。所以 `x=1, y=2, z=102` 是可能的。

**情况二：`get(z)` 返回 0**

`C4` 没有看到 `C2` 写入的结果。因此，`z` 没有提供任何因果信息。`C4` 接下来读 `y` 和 `x`，会看到什么取决于 `C1` 的写入传播了多少。但 `C1` 内部的因果关系 (`put(x,1)` -> `put(y,2)`) 依然要被遵守。
* 可能 `C1` 的两个写都到了：看到 `y=2, x=1`。组合成 `x=1, y=2, z=0`。
* 可能只有 `C1` 的第一个写到了：看到 `y=0, x=1`。组合成 `x=1, y=0, z=0`。
* 不可能只看到 `C1` 的第二个写（`y=2`）而没看到第一个（`x=0`），因为这违反了 `C1` 内部的因果性。

这就是 `C4` 结果受限的原因，它的第一个 `get(z)` 操作“锚定”了一个因果历史。

#### Lab 3 Raft (1): 为什么 Follower 不能直接响应 `Get` 请求？

**解读**

很简单，因为会导致 **陈旧读 (stale reads)** 。

Raft 的核心是通过 `leader` 将所有写操作记录在日志里，并复制给 `follower`。但这个复制过程不是瞬时的，`follower` 的状态很可能落后于 `leader`。

如果一个客户端刚通过 `leader` 执行了一个 `Put("k", "v2")`，成功地把值从 `v1` 改成了 `v2`。然后它马上发起一个 `Get("k")` 请求，但不巧这个请求被发给了一个还没收到最新日志的 `follower`。这个 `follower` 就会从自己的（过时的）状态机里读出 `v1` 并返回给客户端。客户端就会看到一个刚刚被自己修改过的值又变了回去，这破坏了线性一致性。

#### Lab 3 Raft (2): 如何用 TrueTime 让 Follower 安全地响应 `Get` ？

**解读**

这个设计巧妙地把 Spanner 的核心思想嫁接到了 Raft 上，目标是在 `follower` 上实现线性一致性的读。

核心思路是： **确保 `follower` 的状态足够新，能够满足读请求发起时刻的一致性要求。**

1.  **客户端标记请求** ：客户端在发起 `Get` 请求前，先用 `req_ts = TT.now()` 获取一个 **请求时间戳** 。这个时间戳是一个 `[earliest, latest]` 的区间，代表了请求发起的真实时间。客户端将 `req_ts` 连同请求一起发送。
2.  **Leader 标记写入** ：`leader` 在处理 `Put`/`Append` 等写操作时，也会在将命令写入 Raft 日志前，用 `op_ts = TT.now()` 给这条日志条目打上一个 **操作时间戳** 。这个时间戳会随着日志一起被复制给所有 `follower`。
3.  **Follower 等待并响应** ：当一个 `follower` 收到一个带有 `req_ts` 的 `Get` 请求时，它不能立即响应。它必须等待，直到它本地的状态机已经应用了某条日志，而这条日志的操作时间戳 `op_ts` **明确晚于** 读请求的时间戳。
4.  这个判断的精确条件是 `op_ts.earliest > req_ts.latest` 。这个不等式一旦成立，就意味着 `follower` 知道，它当前的状态所反映的真实时间点，一定是在客户端发起读请求之后。
5.  一旦条件满足，`follower` 就可以安全地从自己的本地状态机里读取数据并返回给客户端了，因为它知道自己的数据“足够新”，足以保证这次读的线性一致性。
